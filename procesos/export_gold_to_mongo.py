from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
import sys
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

database='medallon'
collection='gold'

try:
    # Configuraci√≥n expl√≠cita de Spark
    conf = SparkConf() \
        .set("spark.mongodb.connection.uri", "mongodb://172.27.192.1:27017/") \
        .set("spark.mongodb.database", database) \
        .set("spark.mongodb.collection", collection) \
        .set("spark.mongodb.write.mode", "overwrite") \
        .set("spark.mongodb.output.autoIndexCreation", "true")

    # Crear sesi√≥n Spark con paquetes MongoDB
    spark = SparkSession.builder \
        .appName("CSV_to_MongoDB_Gold") \
        .config(conf=conf) \
        .getOrCreate()

    logger.info("‚úÖ Sesi√≥n Spark creada exitosamente")

    # Leer CSV con manejo de errores
    csv_path = "file:/home/hadoop/spark-elt-medallon/datalake/temp/part-*.csv"
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("encoding", "UTF-8") \
        .csv(csv_path)

    logger.info(f"‚úÖ CSV le√≠do desde: {csv_path}")

    # Validar que el DataFrame no est√© vac√≠o
    record_count = df.count()
    logger.info(f"üìä Registros le√≠dos: {record_count}")
    
    if record_count == 0:
        raise ValueError("‚ö†Ô∏è El DataFrame est√° vac√≠o. Verifica el archivo CSV.")

    # Mostrar esquema y muestra de datos (debug)
    logger.info("üîç Esquema del DataFrame:")
    df.printSchema()
    logger.info("üîç Primeras 5 filas:")
    df.show(5, truncate=False)

    # Escribir en MongoDB
    logger.info(f"üíæ Escribiendo en MongoDB: {database}.{collection} ...")
    df.write \
        .format("mongodb") \
        .mode("overwrite") \
        .option("spark.mongodb.database", database) \
        .option("spark.mongodb.collection", collection) \
        .option("spark.mongodb.write.mode", "overwrite") \
        .save()

    logger.info("‚úÖ Datos escritos exitosamente en MongoDB")

    # Verificaci√≥n opcional: contar documentos en MongoDB
    df_verify = spark.read \
        .format("mongodb") \
        .option("spark.mongodb.database", database) \
        .option("spark.mongodb.collection", collection) \
        .load()
    
    logger.info(f"üîç Verificaci√≥n: {df_verify.count()} documentos en {database}.{collection}")

except Exception as e:
    logger.error(f"‚ùå Error durante la ejecuci√≥n: {str(e)}")
    sys.exit(1)

finally:
    spark.stop()
    logger.info("üõë Sesi√≥n Spark cerrada")