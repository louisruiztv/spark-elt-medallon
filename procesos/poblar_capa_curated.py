#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script PySpark para despliegue de capa Curated (Parquet + Snappy + Data Quality)
Adaptado desde Hive SQL original con transformaciones y validaciones de datos
"""

import sys
import argparse
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql.functions import col, cast, when, lit, date_format, to_date
from pyspark.sql.functions import trim, upper, lower, regexp_replace

# =============================================================================
# @section 1. Configuraci√≥n de par√°metros
# =============================================================================

def parse_arguments():
    parser = argparse.ArgumentParser(description='Proceso de carga - Capa Curated')
    parser.add_argument('--env', type=str, default='TopicosB', help='Entorno: DEV, QA, PROD')
    parser.add_argument('--username', type=str, default='hadoop', help='Usuario HDFS')
    parser.add_argument('--base_path', type=str, default='/user', help='Ruta base en HDFS')
    parser.add_argument('--source_db', type=str, default='landing', help='Base de datos origen')
    parser.add_argument('--enable-validation', action='store_true', default=True, help='Activar validaciones de calidad')
    return parser.parse_args()

# =============================================================================
# @section 2. Inicializaci√≥n de SparkSession con configuraci√≥n Hive/Parquet
# =============================================================================

def create_spark_session(app_name="ProcesoCurated-LlanosBardales"):
    return SparkSession.builder \
        .appName(app_name) \
        .enableHiveSupport() \
        .config("spark.sql.parquet.compression.codec", "snappy") \
        .config("spark.sql.sources.partitionOverwriteMode", "dynamic") \
        .config("spark.sql.parquet.writeLegacyFormat", "false") \
        .config("spark.sql.parquet.int96AsTimestamp", "false") \
        .config("hive.exec.compress.output", "true") \
        .config("hive.exec.dynamic.partition", "true") \
        .config("hive.exec.dynamic.partition.mode", "nonstrict") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()

# =============================================================================
# @section 3. Funciones auxiliares para manejo de Parquet y Hive
# =============================================================================

def crear_database(spark, env, username, base_path):
    """Crea la base de datos Curated si no existe"""
    db_name = f"{env}_curated".lower()
    db_location = f"{base_path}/{username}/datalake/{db_name.upper()}"
    
    spark.sql(f"DROP DATABASE IF EXISTS {db_name} CASCADE")
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '{db_location}'")
    print(f"‚úÖ Database '{db_name}' creada en: {db_location}")
    return db_name

def crear_tabla_parquet_hive(spark, db_name, table_name, schema_spark, location, partitioned_by=None):
    """
    Crea tabla externa en Hive con formato Parquet usando SQL directo
    para mantener compatibilidad total con propiedades Hive
    """
    # Construir definici√≥n de columnas
    columns_def = []
    for field in schema_spark.fields:
        col_def = f"{field.name} {field.dataType.simpleString().upper()}"
        columns_def.append(col_def)
    
    partition_clause = ""
    if partitioned_by:
        partition_cols = ", ".join([f"{c} STRING" for c in partitioned_by])
        partition_clause = f"PARTITIONED BY ({partition_cols})"
    
    create_sql = f"""
    CREATE TABLE IF NOT EXISTS {db_name}.{table_name} (
        {', '.join(columns_def)}
    )
    {partition_clause}
    STORED AS PARQUET
    LOCATION '{location}'
    TBLPROPERTIES (
        'store.charset'='ISO-8859-1',
        'retrieve.charset'='ISO-8859-1',
        'parquet.compression'='SNAPPY'
    )
    """
    spark.sql(create_sql)
    print(f"‚úÖ Tabla Parquet '{db_name}.{table_name}' registrada en Hive Metastore")

def aplicar_reglas_calidad_persona(df, enable_validation=True):
    """
    Aplica transformaciones y reglas de calidad para tabla PERSONA
    - Casteo de tipos
    - Validaciones de rangos y nulls
    """
    df_transformed = df.select(
        col("ID").cast(StringType()).alias("ID"),
        col("NOMBRE").cast(StringType()).alias("NOMBRE"),
        col("TELEFONO").cast(StringType()).alias("TELEFONO"),
        col("CORREO").cast(StringType()).alias("CORREO"),
        col("FECHA_INGRESO").cast(StringType()).alias("FECHA_INGRESO"),
        col("EDAD").cast(IntegerType()).alias("EDAD"),
        col("SALARIO").cast(DoubleType()).alias("SALARIO"),
        col("ID_EMPRESA").cast(StringType()).alias("ID_EMPRESA")
    )
    
    if enable_validation:
        # Aplicar filtros de calidad
        df_transformed = df_transformed.filter(
            (col("ID").isNotNull()) &
            (col("ID_EMPRESA").isNotNull()) &
            (col("EDAD").between(1, 99)) &
            (col("SALARIO").between(0.01, 9999999.99))
        )
    
    return df_transformed

def aplicar_reglas_calidad_empresa(df, enable_validation=True):
    """Aplica transformaciones y reglas de calidad para tabla EMPRESA"""
    df_transformed = df.select(
        col("ID").cast(StringType()).alias("ID"),
        col("NOMBRE").cast(StringType()).alias("NOMBRE")
    )
    
    if enable_validation:
        df_transformed = df_transformed.filter(col("ID").isNotNull())
    
    return df_transformed

def aplicar_reglas_calidad_transaccion(df, enable_validation=True):
    """Aplica transformaciones y reglas de calidad para tabla TRANSACCION"""
    df_transformed = df.select(
        col("ID_PERSONA").cast(StringType()).alias("ID_PERSONA"),
        col("ID_EMPRESA").cast(StringType()).alias("ID_EMPRESA"),
        col("MONTO").cast(DoubleType()).alias("MONTO"),
        col("FECHA").cast(StringType()).alias("FECHA")  # Columna de partici√≥n
    )
    
    if enable_validation:
        df_transformed = df_transformed.filter(
            (col("ID_PERSONA").isNotNull()) &
            (col("ID_EMPRESA").isNotNull()) &
            (col("MONTO") >= 0)
        )
    
    return df_transformed

def insertar_datos_parquet(spark, db_name, table_name, df_transformed, partition_col=None):

    df_transformed.write \
        .mode("overwrite") \
        .format("parquet") \
        .option("compression", "snappy") \
        .insertInto(f"{db_name}.{table_name}", overwrite=True)

    print(f"‚úÖ Datos insertados correctamente en {db_name}.{table_name}")

# =============================================================================
# @section 4. Definici√≥n de esquemas para capa Curated
# =============================================================================

SCHEMAS_CURATED = {
    "PERSONA": StructType([
        StructField("ID", StringType(), False),
        StructField("NOMBRE", StringType(), True),
        StructField("TELEFONO", StringType(), True),
        StructField("CORREO", StringType(), True),
        StructField("FECHA_INGRESO", StringType(), True),
        StructField("EDAD", IntegerType(), True),
        StructField("SALARIO", DoubleType(), True),
        StructField("ID_EMPRESA", StringType(), False)
    ]),
    "EMPRESA": StructType([
        StructField("ID", StringType(), False),
        StructField("NOMBRE", StringType(), True)
    ]),
    "TRANSACCION": StructType([
        StructField("ID_PERSONA", StringType(), False),
        StructField("ID_EMPRESA", StringType(), False),
        StructField("MONTO", DoubleType(), True)
        # FECHA es columna de partici√≥n, no va en el schema principal
    ])
}

# Configuraci√≥n de tablas
TABLAS_CONFIG = [
    {
        "nombre": "PERSONA",
        "partitioned_by": None,
        "func_calidad": aplicar_reglas_calidad_persona
    },
    {
        "nombre": "EMPRESA",
        "partitioned_by": None,
        "func_calidad": aplicar_reglas_calidad_empresa
    },
    {
        "nombre": "TRANSACCION",
        "partitioned_by": ["FECHA"],
        "func_calidad": aplicar_reglas_calidad_transaccion
    }
]

# =============================================================================
# @section 5. Proceso principal de carga Curated
# =============================================================================

def procesar_tabla_curated(spark, args, db_curated, db_source, config):
    """Procesa una tabla: aplica transformaciones, calidad y carga en Parquet"""
    
    table_name = config["nombre"]
    print(f"üîÑ Procesando tabla Curated: {table_name}")
    
    # Rutas
    location = f"{args.base_path}/{args.username}/datalake/{db_curated.upper()}/{table_name}"
    
    # 1. Crear tabla en Hive Metastore con formato Parquet
    crear_tabla_parquet_hive(
        spark=spark,
        db_name=db_curated,
        table_name=table_name,
        schema_spark=SCHEMAS_CURATED[table_name],
        location=location,
        partitioned_by=config["partitioned_by"]
    )
    
    # 2. Leer datos desde capa landing
    source_table = f"{db_source}.{table_name}"
    df_source = spark.table(source_table)
    print(f"   üìä Registros origen: {df_source.count()}")
    
    # 3. Aplicar transformaciones y reglas de calidad
    func_calidad = config["func_calidad"]
    df_transformed = func_calidad(df_source, enable_validation=args.enable_validation)
    print(f"   ‚ú® Registros despu√©s de calidad: {df_transformed.count()}")
    
    # 4. Insertar datos con compresi√≥n Snappy
    partition_col = config["partitioned_by"][0] if config["partitioned_by"] else None
    insertar_datos_parquet(
        spark=spark,
        db_name=db_curated,
        table_name=table_name,
        df_transformed=df_transformed,
        partition_col=partition_col
    )
    
    # 5. Validaci√≥n y muestra de resultados
    print(f"üîç Muestra de datos - {table_name}:")
    spark.sql(f"SELECT * FROM {db_curated}.{table_name} LIMIT 10").show(truncate=False)
    
    # 6. Si est√° particionada, mostrar particiones
    if config["partitioned_by"]:
        print(f"üìÅ Particiones creadas:")
        try:
            spark.sql(f"MSCK REPAIR TABLE {db_curated}.{table_name}")
            spark.sql(f"SHOW PARTITIONS {db_curated}.{table_name}").show(truncate=False)
        except Exception as e:
            print(f"   ‚ö†Ô∏è  No se pudieron listar particiones: {e}")
    
    # 7. Estad√≠sticas de calidad
    if args.enable_validation:
        total_origen = spark.table(source_table).count()
        total_curated = spark.sql(f"SELECT COUNT(*) FROM {db_curated}.{table_name}").first()[0]
        rechazados = total_origen - total_curated
        porcentaje = (total_curated / total_origen * 100) if total_origen > 0 else 0
        print(f"üìà Calidad: {total_curated}/{total_origen} registros aceptados ({porcentaje:.2f}%)")
        if rechazados > 0:
            print(f"   ‚ö†Ô∏è  {rechazados} registros filtrados por reglas de calidad")
    
    print("-" * 80)

# =============================================================================
# @section 6. Funciones de monitoreo y auditor√≠a
# =============================================================================

def generar_reporte_calidad(spark, db_curated, db_source, timestamp):
    """Genera un reporte de calidad de datos entre capas"""
    print("\nüìã REPORTE DE CALIDAD DE DATOS")
    print("=" * 80)
    
    reporte = []
    for config in TABLAS_CONFIG:
        tbl = config["nombre"]
        try:
            src_count = spark.sql(f"SELECT COUNT(*) FROM {db_source}.{tbl}").first()[0]
            tgt_count = spark.sql(f"SELECT COUNT(*) FROM {db_curated}.{tbl}").first()[0]
            calidad = (tgt_count / src_count * 100) if src_count > 0 else 100.0
            
            reporte.append({
                "tabla": tbl,
                "origen": src_count,
                "curated": tgt_count,
                "calidad_pct": calidad
            })
            print(f"‚Ä¢ {tbl:12} | Origen: {src_count:6} | Curated: {tgt_count:6} | Calidad: {calidad:5.2f}%")
        except Exception as e:
            print(f"‚Ä¢ {tbl:12} | ‚ùå Error: {e}")
    
    # Guardar reporte en tabla de auditor√≠a (opcional)
    try:
        from pyspark.sql.types import TimestampType
        df_reporte = spark.createDataFrame(reporte)
        df_reporte = df_reporte.withColumn("ejecucion", lit(timestamp))
        
        if spark.catalog.tableExists(f"{db_curated}.auditoria_calidad"):
            df_reporte.write.mode("append").saveAsTable(f"{db_curated}.auditoria_calidad")
        else:
            df_reporte.write.mode("overwrite").saveAsTable(f"{db_curated}.auditoria_calidad")
        print(f"‚úÖ Reporte guardado en {db_curated}.auditoria_calidad")
    except Exception as e:
        print(f"‚ö†Ô∏è  No se pudo guardar reporte de auditor√≠a: {e}")
    
    print("=" * 80)

# =============================================================================
# @section 7. Ejecuci√≥n principal
# =============================================================================

def main():
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    args = parse_arguments()
    spark = create_spark_session()
    
    try:
        # Configurar entorno
        env_lower = args.env.lower()
        db_curated = f"{env_lower}_curated"
        db_source = f"{env_lower}_{args.source_db}"
        
        print(f"üöÄ Iniciando proceso Curated | Env: {args.env.upper()} | Usuario: {args.username}")
        print(f"üìÅ Base path: {args.base_path}")
        print("-" * 80)
        
        # 1. Crear database Curated
        crear_database(spark, env_lower, args.username, args.base_path)
        
        # 2. Procesar cada tabla configurada
        for config in TABLAS_CONFIG:
            procesar_tabla_curated(
                spark=spark,
                args=args,
                db_curated=db_curated,
                db_source=db_source,
                config=config
            )
        
        # 3. Generar reporte de calidad
        generar_reporte_calidad(spark, db_curated, db_source, timestamp)
        
        # 4. Resumen final
        print("\nüéâ Proceso de Curated completado exitosamente!")
        print(f"üìä Tablas disponibles en {db_curated}:")
        spark.sql(f"SHOW TABLES IN {db_curated}").show(truncate=False)
        
        # 5. Verificaci√≥n de formato y compresi√≥n
        print("\nüîç Verificaci√≥n de almacenamiento Parquet + Snappy:")
        for config in TABLAS_CONFIG:
            tbl = f"{db_curated}.{config['nombre']}"
            try:
                result = spark.sql(f"DESCRIBE FORMATTED {tbl}")
                format_info = result.filter(
                    col("col_name").isin("InputFormat", "OutputFormat", "Serde Library")
                ).collect()
                print(f"  ‚Ä¢ {config['nombre']}: Parquet + Snappy ‚úÖ")
            except:
                print(f"  ‚Ä¢ {config['nombre']}: ‚ö†Ô∏è  No se pudo verificar")
        
    except Exception as e:
        print(f"‚ùå Error cr√≠tico en el proceso: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        spark.stop()
        print("üîö Sesi√≥n de Spark cerrada")

if __name__ == "__main__":
    main()